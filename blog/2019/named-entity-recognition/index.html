<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Demystifying Named Entity Recognition - Part I | Kehang Han </title> <meta name="author" content="Kehang Han"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kehang.github.io/blog/2019/named-entity-recognition/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Kehang</span> Han </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Demystifying Named Entity Recognition - Part I</h1> <p class="post-meta"> Created in May 14, 2019 </p> <p class="post-tags"> <a href="/blog/2019"> <i class="fa-solid fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/category/nlp"> <i class="fa-solid fa-tag fa-sm"></i> nlp,</a>   <a href="/blog/category/explained"> <i class="fa-solid fa-tag fa-sm"></i> explained</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Recently I’ve been working on a project related to <strong>Named Entity Recognition</strong> (<strong>NER</strong>). At the very beginning, I was trying to find a well-explained document to get myself started, but couldn’t do so (instead I found redundant pieces here and there on the Internet). My requirement is simple. It should include</p> <ul> <li> <p><em>what</em> is <strong>NER</strong></p> </li> <li> <p><em>how</em> to <strong>formulate</strong> it</p> </li> <li> <p><em>what</em> are the <strong>traditional</strong> and <strong>start-of-the-art</strong> models</p> </li> <li> <p><em>what</em> are the <strong>off-the-shelf</strong> options</p> </li> <li> <p><em>how</em> to build a <strong>customized</strong> NER model</p> </li> </ul> <p>So this post will try to provide a complete set of explanation on these questions.</p> <h2 id="1-what-is-ner">1. What is NER</h2> <p>Simply put, <strong>Named Entity Recognition</strong> is a technology that identifies <strong>certain entities</strong> from a sentence/paragraph/document. Like the sentence below, <strong>NER</strong> tries to tag each word with a label; <code class="language-plaintext highlighter-rouge">Steve Jobs</code> and <code class="language-plaintext highlighter-rouge">Steve Wozniak</code> to be <code class="language-plaintext highlighter-rouge">PER</code> (persion entity), <code class="language-plaintext highlighter-rouge">Apple</code> to be <code class="language-plaintext highlighter-rouge">ORG</code> (organization entity) and the rest <code class="language-plaintext highlighter-rouge">O</code> (not an entity).</p> <p class="srs_img"><img src="/assets/ner_post/img/what-is-ner.png" alt="Alt text" width="80%"></p> <p>This is useful in lots of cases. For instance, <em>Apple Mail</em> identifies <code class="language-plaintext highlighter-rouge">TIME</code> entity in emails and makes pulling events from email to calendar much easier than before; <em>Google Search</em> is able to find <code class="language-plaintext highlighter-rouge">Headquarters</code> entity in a relevent document to answer query questions.</p> <p class="srs_img"><img src="/assets/ner_post/img/what-is-ner-2.png" alt="Alt text" width="80%"></p> <p>So how does it actually work?</p> <h2 id="2-mathematical-formulation">2. Mathematical Formulation</h2> <p>Let’s step back and see what <strong>NER</strong> is doing essentially. For a given sentence \(x_1 ... x_n\), <strong>NER</strong> decides to tag each word \(x_i\) with an entity label \(y_i\).</p> <p class="srs_img"><img src="/assets/ner_post/img/math-formulation-1.png" alt="Alt text" width="80%"></p> <p>One obvious challenge here is one word should be tagged differently depending on its <strong>context</strong>. E.g., Apple in the example above is an organization entity, but in other contexts might be a fruit entity.</p> <p>So how can we tag wisely? Basically we need two things.</p> <ul> <li> <p>a <strong>score function</strong> \(s(y_1...y_n, x_1...x_n)\), which splits out a score measuring how fit a tagging sequence \(y_1 ... y_n\) is to a given sentence \(x_1 ... x_n\). A well-designed score function should assign a higher score to a tagging sequence that fits better.</p> </li> <li> <p>a <strong>solver</strong>, which is able to pick the highest-scoring tagging sequence among overwhelmingly large number of possible candidates. Just take a sentence with \(7\) words as an example, we are talking about \(10^7\) tagging candidates if there’s 10 unique entities to choose from. A good solver should be able to efficiently get to the best tagging sequence.</p> </li> </ul> <h3 id="21-score-function">2.1 Score Function</h3> <p>Researchers in the field like to use probability model to build score function: a better-fitting sequence can be given a higher probability. Often times we choose <strong>conditional probability</strong> (one can use joint probablity as well), like below</p> \[s(y_1...y_n, x_1...x_n) = p(y_1...y_n | x_1...x_n) = \prod_{i=1}^{n} p(y_i |y_1...y_{i-1}, x_1...x_n)\] <p>If we make a simplication \(p(y_i |y_1...y_{i-1}, x_1...x_n) \approx p(y_i | y_{i-1}, x_1...x_n)\), then</p> \[s(y_1...y_n, x_1...x_n) = \prod_{i=1}^{n} p(y_i |y_{i-1}, x_1...x_n)\] <p>Now the question becomes how do we model</p> \[p(y_i | y_{i-1}, x_1...x_n)\] <h4 id="211-model-py_i--y_i-1-x_1x_n">2.1.1 Model \(p(y_i | y_{i-1}, x_1...x_n)\)</h4> <p>Following the above example, we are basically asking <em>how likely Jobs is PER / ORG / MISC / O, if Steve is PER?</em></p> <p class="srs_img"><img src="/assets/ner_post/img/math-formulation-2.png" alt="Alt text" width="80%"></p> <p>A natural thought would be to create a <strong>second</strong> score function (let’s call it local score function since it looks only at \(y_i, y_{i-1}\)) with well-defined local features \(\vec{f}(y_{i-1}, y_i, x_1...x_n)\):</p> \[s_{local}(y_{i-1}, y_i, x_1...x_n) = \vec{\theta} \cdot \vec{f}(y_{i-1}, y_i, x_1...x_n)\] <p>and define probability based on the local score function:</p> \[p(y_i | y_{i-1}, x_1...x_n) = \frac{\exp{s_{local}(y_{i-1}, y_i, x_1...x_n)}}{\sum_{y'}{\exp{s_{local}(y_{i-1}, y', x_1...x_n)}}} = \frac{\exp({\vec{\theta} \cdot \vec{f}(y_{i-1}, y_i, x_1...x_n)})}{\sum_{y'}{\exp({\vec{\theta} \cdot \vec{f}(y_{i-1}, y', x_1...x_n)})}}\] <p>\(\vec{f} \in R^m\), is an m-dimension vector, meaning there’s <strong>m</strong> predefined feature functions \(f_1...f_m\). Some feature examples could be:</p> \[\begin{align*} f_1(y_{i-1}, y_i, x_1...x_n) = \left\{ \begin{array}{ccc} 1 &amp; \text{if $x_{i-1}, x_i$ are capitalized, $y_{i-1}, y_i=\text{PER}$}\\ 0 &amp; \text{otherwise} \end{array} \right. \end{align*}\] \[\begin{align*} f_2(y_{i-1}, y_i, x_1...x_n) = \left\{ \begin{array}{ccc} 1 &amp; \text{if $x_i$ ends in ing, $y_i=\text{O}$}\\ 0 &amp; \text{otherwise} \end{array} \right. \end{align*}\] \[...\] <p>At this point, once we have defined \(\vec{f}\) and picked the weights \(\theta\) for the features, we can readily calculate \(p(y_i | y_{i-1}, x_1...x_n)\) and tagging score from</p> \[p(y_i | y_{i-1}, x_1...x_n) = \frac{\exp({\vec{\theta} \cdot \vec{f}(y_{i-1}, y_i, x_1...x_n)})}{\sum_{y'}{\exp({\vec{\theta} \cdot \vec{f}(y_{i-1}, y', x_1...x_n)})}}\] \[s(y_1...y_n, x_1...x_n) = \prod_{i=1}^{n} \frac{\exp({\vec{\theta} \cdot \vec{f}(y_{i-1}, y_i, x_1...x_n)})}{\sum_{y'}{\exp({\vec{\theta} \cdot \vec{f}(y_{i-1}, y', x_1...x_n)})}}\] <h4 id="212-three-remaining-questions">2.1.2 Three remaining questions</h4> <p>Let’s summarize here. With this framework set up, there’s only <strong>3</strong> questions remaining:</p> <ul> <li>how to design feature functions \(\vec{f}(y_{i-1}, y_i, x_1...x_n)\)?</li> </ul> <blockquote> <p>Traditionally it’s kinda art and requires expert knowledge in the problem domain. But recent deep learning approaches such as LSTM, BiLSTM aim to utilize large neural networks and automatically figure out suitable featurization from data.</p> </blockquote> <ul> <li>how to estimate weights \(\vec{\theta}\)?</li> </ul> <blockquote> <p>\(\vec{\theta}\) can be obtained by training on data. One intuitive and popularly used method is <em>Maximum Likelihood Estimation</em>.</p> </blockquote> <ul> <li>assuming we already know \(\vec{f}, \vec{\theta}\), how to get the best fitting tag sequence \(y_1^{*}...y_n^{*}\) for a sentence \(x_1...x_n\)?</li> </ul> <blockquote> <p>This will be the central topic in the next section - Solver section.</p> </blockquote> <h3 id="22-solver">2.2 Solver</h3> <p>Now let’s assume we’ve determined \(\vec{f}, \vec{\theta}\) from expert knowledge and/or data, so our score function \(s(y_1...y_n, x_1...x_n)\) is finalized.</p> <p>The job of the solver here is use the score function to efficiently get the best fitting tag sequence \(y_1^{*}...y_n^{*}\) for a given sentence \(x_1...x_n\). It’s an optimization problem:</p> \[y_1^{*}...y_n^{*} = \text{arg} \max_{y_1...y_n} s(y_1...y_n, x_1...x_n)\] <p>We can definitely solve it by brutal force - it’s just that there’s \(5^7\) possible \(y_1...y_n\) sequences in the example below, meaning we have to evaluate \(5^7\) times of our score function before getting the optimal sequence.</p> <p class="srs_img"><img src="/assets/ner_post/img/solver-1.png" alt="Alt text" width="80%"></p> <p>The computation scales badly with the length of sentence.</p> <blockquote> <p>The computation complexity is exponential - \(O(5^N)\) where \(N\) is the length of sentence.</p> </blockquote> <p>It turns out the score function exhibits a special mathematic structure that enables us to solve the optimization problem in linear time \(O(N)\). Let’s talk about that.</p> <h4 id="221-viterbi-algorithm">2.2.1 Viterbi Algorithm</h4> <p>We notice our score function is a product of successive conditional probabilities \(p(y_i |y_{i-1}, x_1...x_n)\) as below:</p> \[s(y_1...y_n, x_1...x_n) = \prod_{i=1}^{n} p(y_i |y_{i-1}, x_1...x_n)\] <blockquote> <p>You’ll see this is a very good property that allows us to optimize step by step via dynamic programming (in this case we call Viterbi Algorithm). So, bear with me for a bit…</p> </blockquote> <p>Because of this nice and clean form, we can easily define a partial score function \(s_{partial}\) - score of the first \(k\) tags \(y_1...y_k\) for the same given sentence \(x_1...x_n\).</p> \[s_{partial, k}(y_1...y_k, x_1...x_n) = \prod_{i=1}^{k} p(y_i |y_{i-1}, x_1...x_n)\] <p>When \(k=n\), the partial score function becomes the original score function.</p> <p class="srs_img"><img src="/assets/ner_post/img/solver-2.png" alt="Alt text" width="80%"></p> <p>We can also easily find the <strong>recursive form</strong> of the partial score function:</p> \[s_{partial, k}(y_1...y_k, x_1...x_n) = p(y_k |y_{k-1}, x_1...x_n) \cdot s_{partial, k-1}(y_1...y_{k-1}, x_1...x_n)\] <p>Now, we only need answer <strong>three</strong> questions to explain the algorithm.</p> <ul> <li>How would you find highest partial score where \(k=1\), namely \(s_{partial, 1}(y_1^{*}, x_1...x_n)\)?</li> </ul> <blockquote> <p>This is easy because it’s just a single-variable optimization and \(y_1\) can only have 5 choices.</p> </blockquote> <p>We can just evaluate \(s_{partial, 1}(y_1, x_1...x_n)\) for 5 times and pick the highest one. To help answer the following questions, we’ll save the 5 evaluated numbers. Here we carried out \(5\) evalutions to get \(s_{partial, 1}(y_1^{*}, x_1...x_n)\).</p> <ul> <li>How would you find highest partial score where \(k=2\), namely \(s_{partial, 2}(y_1^{*}, y_2^{*}, x_1...x_n)\)?</li> </ul> <blockquote> <p>It’s just bi-variable optimization. We can first keep \(y_2\) fixed and optimize over \(y_1\) dimension.</p> </blockquote> <p>We can calculate the best \(s_{partial, 2}(y_1^{*},y_2=\text{PER}, x_1...x_n)\) by evaluating the following 5 products and pick the best one.</p> <ol> <li> \[p(y_2=\text{PER} |y_1=\text{PER}, x_1...x_n) \cdot s_{partial, 1}(y_1=\text{PER}, x_1...x_n)\] </li> <li> \[p(y_2=\text{PER} |y_1=\text{ORG}, x_1...x_n) \cdot s_{partial, 1}(y_1=\text{ORG}, x_1...x_n)\] </li> <li> \[p(y_2=\text{PER} |y_1=\text{LOC}, x_1...x_n) \cdot s_{partial, 1}(y_1=\text{LOC}, x_1...x_n)\] </li> <li> \[p(y_2=\text{PER} |y_1=\text{MISC}, x_1...x_n) \cdot s_{partial, 1}(y_1=\text{MISC}, x_1...x_n)\] </li> <li> \[p(y_2=\text{PER} |y_1=\text{O}, x_1...x_n) \cdot s_{partial, 1}(y_1=\text{O}, x_1...x_n)\] </li> </ol> <blockquote> <p>Note we can reuse \(s_{partial, 1}(y_1, x_1...x_n), y_1 \in \{\text{PER, ORG, LOC, MISC, O}\}\) saved from previous question.</p> </blockquote> <p>In this manner, we have all of \(s_{partial, 2}(y_1^{*},y_2, x_1...x_n), y_2 \in \{\text{PER, ORG, LOC, MISC, O}\}\) ready and the highest of the five would be the answer. Here we carried out \(5*5=25\) evalutions to get \(s_{partial, 2}(y_1^{*}, y_2^{*}, x_1...x_n)\).</p> <ul> <li>How about \(k=3\) and beyond?</li> </ul> <p>At this moment, you might have noticed that we could get \(s_{partial, 3}(y_1^{*}, y_2^{*}, y_3^{*}, x_1...x_n)\) by evaluating \(s_{partial, 3}(y_1^{*},y_2^{*}, y_3, x_1...x_n), y_3 \in \{\text{PER, ORG, LOC, MISC, O}\}\) and picking the highest of the five.</p> <p>Each \(s_{partial, 3}(y_1^{*},y_2^{*}, y_3=u, x_1...x_n)\) can be easily calculated via</p> \[\max_{y_2} p(y_3=u |y_2, x_1...x_n) \cdot s_{partial, 2}(y_1^{*}, y_2, x_1...x_n)\] <blockquote> <p>Note we can reuse \(s_{partial, 2}(y_1^{*}, y_2, x_1...x_n), y_2 \in \{\text{PER, ORG, LOC, MISC, O}\}\) saved from previous question.</p> </blockquote> <p>Similarly, it takes another 25 evalutions. Basically we can keep rolling to all the way \(k=n\) with each step forward we carry out another 25 evaluations. So the total compuation complexity is around \(25N\), namely \(O(N)\).</p> <p>This post turns out to be longer than I thought, I was only able to answer the first questions raised in the beginning. And this seemingly natural approach is actually the so-called <em>maximum entropy Markov model</em> (MEMM) approach. There’s many more other ways to solve NER problem, which I’ll talk about in next posts together with the answers to the remaining three questions. See you soon.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/battle-tested-llm-training-input-pipeline/">Battle-Tested LLM Training: The Input Pipeline</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/a-desk-that-listens/">A Desk That Listens</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/a-desk-with-its-own-schedule/">A Desk with Its Own Schedule</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/named-entity-recognition-part2/">Demystifying Named Entity Recognition - Part II</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/machine-learning-in-molecular-property-prediction/">Molecular ConvNet in Property Prediction</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Kehang Han. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: August 04, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-battle-tested-llm-training-the-input-pipeline",title:"Battle-Tested LLM Training: The Input Pipeline",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/battle-tested-llm-training-input-pipeline/"}},{id:"post-a-desk-that-listens",title:"A Desk That Listens",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/a-desk-that-listens/"}},{id:"post-a-desk-with-its-own-schedule",title:"A Desk with Its Own Schedule",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/a-desk-with-its-own-schedule/"}},{id:"post-demystifying-named-entity-recognition-part-ii",title:"Demystifying Named Entity Recognition - Part II",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/named-entity-recognition-part2/"}},{id:"post-demystifying-named-entity-recognition-part-i",title:"Demystifying Named Entity Recognition - Part I",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/named-entity-recognition/"}},{id:"post-molecular-convnet-in-property-prediction",title:"Molecular ConvNet in Property Prediction",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/machine-learning-in-molecular-property-prediction/"}},{id:"post-6-036-project-2-mnist-classifiers",title:"6.036 Project 2: MNIST Classifiers",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/mnist-classifiers-exploration/"}},{id:"post-install-cuda-and-cudnn-on-red-hat",title:"Install CUDA and cuDNN on Red Hat",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/install-CUDA-cuDNN-on-Red-Hat/"}},{id:"post-smart-review-summarization-project",title:"Smart Review Summarization Project",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/smart-review-summarization-project/"}},{id:"post-reactor-optimization-with-fmincon",title:"Reactor Optimization with fmincon!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/fmincon-tutorial/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6B%65%68%61%6E%67%68%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=ON8jkO0AAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>