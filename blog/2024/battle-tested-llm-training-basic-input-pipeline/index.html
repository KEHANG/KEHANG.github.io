<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Battle-Tested LLM Training: From Dataset to Data Iterator | Kehang Han </title> <meta name="author" content="Kehang Han"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kehang.github.io/blog/2024/battle-tested-llm-training-basic-input-pipeline/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Kehang</span> Han </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/resume_2024.pdf" target="_blank">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Battle-Tested LLM Training: From Dataset to Data Iterator</h1> <p class="post-meta"> Created in August 11, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>If you find an interesting dataset (often from either Huggingface or TFDS nowadays) and you’d like to use it for LLM training, this post is for you! Specifically, I’ll be explaining the process that gradually turns a Huggingface dataset to an iterator that’s ready to feed model training with batches of data. Conceptually it takes four steps.</p> <p><img src="/assets/battle_tested_llm_training/input_pipeline_data2iter.png" alt="Alt text" width="85%"></p> <p>To make it concrete, I’ll use MaxText’s <a href="https://github.com/google/maxtext/blob/da50760ac0baf3920305a365215f6f0c5f110ad2/MaxText/input_pipeline/_hf_data_processing.py#L121" rel="external nofollow noopener" target="_blank">make_hf_iterator</a> as my reference code, and choose <a href="https://huggingface.co/datasets/stas/openwebtext-10k" rel="external nofollow noopener" target="_blank">openwebtext-10k</a> as our input dataset.</p> <h3 id="load-raw-dataset">load raw dataset</h3> <p>First, let’s load the raw <code class="language-plaintext highlighter-rouge">openwebtext-10k</code> dataset. If <code class="language-plaintext highlighter-rouge">streaming</code> is on as shown below, data files will not be downloaded. Instead, it streams the data progressively while iterating on the dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">stas/openwebtext-10k</span><span class="sh">"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">,</span> <span class="n">streaming</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h3 id="tokenize">tokenize</h3> <p>At this stage, we need tokenize the raw dataset’s <code class="language-plaintext highlighter-rouge">text</code> field and trim the tokenized sequence up to predefined <code class="language-plaintext highlighter-rouge">max_length</code>. Practically, we’d first create a tokenizer either from a local file or from Huggingface via <code class="language-plaintext highlighter-rouge">tokenizer_path</code>. In the example below, we use <code class="language-plaintext highlighter-rouge">t5-small</code> tokenizer, which would be fetched from Huggingface directly.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sets some constants
</span><span class="n">add_bos</span><span class="p">,</span> <span class="n">add_eos</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="mi">512</span>
<span class="n">tokenizer_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">t5-small</span><span class="sh">"</span>
<span class="n">data_column_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span>

<span class="c1"># Creates a tokenizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">tokenizer_path</span><span class="p">,</span>
    <span class="n">add_bos_token</span><span class="o">=</span><span class="n">add_bos</span><span class="p">,</span>
    <span class="n">add_eos_token</span><span class="o">=</span><span class="n">add_eos</span><span class="p">,</span>
    <span class="n">model_max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
    <span class="n">legacy</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <p>Tokenization is then accomplished by running that tokenizer via dataset <code class="language-plaintext highlighter-rouge">map</code> function, <a href="https://github.com/google/maxtext/blob/ed21f6ad1bc60285958d585753dda01e1ddfa664/MaxText/input_pipeline/_input_pipeline_utils.py#L56" rel="external nofollow noopener" target="_blank">_input_pipeline_utils.tokenization</a>. This function applies the above tokenizer to the field <code class="language-plaintext highlighter-rouge">data_column_name</code> of each data example and truncates the tokens up to <code class="language-plaintext highlighter-rouge">max_length</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">maxtext.MaxText.input_pipeline</span> <span class="kn">import</span> <span class="n">_input_pipeline_utils</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
    <span class="n">_input_pipeline_utils</span><span class="p">.</span><span class="n">tokenization</span><span class="p">,</span>
    <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">fn_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">hf_tokenizer</span><span class="sh">"</span><span class="p">:</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="sh">"</span><span class="s">max_length</span><span class="sh">"</span><span class="p">:</span> <span class="n">max_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">column_name</span><span class="sh">"</span><span class="p">:</span> <span class="n">data_column_name</span><span class="p">},</span>
<span class="p">)</span>
<span class="c1"># Post-tokenization: renaming the field where the tokens are.
</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">select_columns</span><span class="p">([</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">]).</span><span class="nf">rename_column</span><span class="p">(</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">,</span> <span class="n">data_column_name</span><span class="p">)</span>
</code></pre></div></div> <h3 id="transform-pack-and-shift">transform: pack and shift</h3> <p>After tokenization, data examples become token sequences of various lengths. To increase training efficiency, we try to pack as many sequences as possible into the context window (<code class="language-plaintext highlighter-rouge">max_length</code> in the code). Here we use grain’s experimental packing API <a href="https://github.com/google/grain/blob/9b984e8a2ccdd3d377cb43b17591080b44c07009/grain/_src/python/experimental/example_packing/packing.py#L152" rel="external nofollow noopener" target="_blank">PackAndBatchOperation</a>.</p> <p>In multi-host setting, each host (i.e., process) gets an equal share of the global batch size (say <code class="language-plaintext highlighter-rouge">512</code>), and this input pipeline code runs at host-level in parallel, thus we want host-level batch size when we batch, i.e., <code class="language-plaintext highlighter-rouge">global_batch_size // jax.process_count()</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">grain.python</span> <span class="k">as</span> <span class="n">grain</span>
<span class="c1"># Sets some constants.
</span><span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">512</span>

<span class="c1"># Adds packing transformation.
</span><span class="n">transformations</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># HFNormalizeFeatures makes two copies of `text` field: one is called
# `inputs` and the other `targets`.
</span><span class="n">operations</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">_input_pipeline_utils</span><span class="p">.</span><span class="nc">HFNormalizeFeatures</span><span class="p">(</span><span class="n">data_column_name</span><span class="p">))</span>
<span class="n">transformations</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
    <span class="n">grain</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="nc">PackAndBatchOperation</span><span class="p">(</span>
        <span class="c1"># In multi-host setting, each host (i.e., process) gets an equal share 
</span>        <span class="c1"># of the global batch size.
</span>        <span class="c1"># And this input pipeline runs at host-level in parallel, thus we want 
</span>        <span class="c1"># host-level batch size here.
</span>        <span class="n">batch_size</span><span class="o">=</span><span class="n">global_batch_size</span> <span class="o">//</span> <span class="n">jax</span><span class="p">.</span><span class="nf">process_count</span><span class="p">(),</span>
        <span class="n">length_struct</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">inputs</span><span class="sh">"</span><span class="p">:</span> <span class="n">max_length</span><span class="p">,</span> <span class="sh">"</span><span class="s">targets</span><span class="sh">"</span><span class="p">:</span> <span class="n">max_length</span><span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Post-packing: reformating tuple to flat dict style.
</span><span class="n">transformations</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">_input_pipeline_utils</span><span class="p">.</span><span class="nc">ReformatPacking</span><span class="p">())</span>
</code></pre></div></div> <p>Finally we shift the <code class="language-plaintext highlighter-rouge">inputs</code> field by 1 token to the right, to make it ready for teacher-forcing training.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">transformations</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">_input_pipeline_utils</span><span class="p">.</span><span class="nc">ShiftData</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div> <h3 id="sample">sample</h3> <p>Now with all the transformations done, we need to tell each host how to sample from the transformed dataset. Most common settings include number of epochs (<code class="language-plaintext highlighter-rouge">num_epochs</code>), which shard of the dataset the current host should load (<code class="language-plaintext highlighter-rouge">shard_options</code>), whether to shuffle (<code class="language-plaintext highlighter-rouge">shuffle</code>), etc.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sampler</span> <span class="o">=</span> <span class="n">grain</span><span class="p">.</span><span class="nc">IndexSampler</span><span class="p">(</span>
    <span class="n">num_records</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">shard_options</span><span class="o">=</span><span class="n">grain</span><span class="p">.</span><span class="nc">ShardOptions</span><span class="p">(</span>
        <span class="n">shard_index</span><span class="o">=</span><span class="n">dataloading_host_index</span><span class="p">,</span> <span class="n">shard_count</span><span class="o">=</span><span class="n">dataloading_host_count</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">),</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <h3 id="put-together">put together</h3> <p>We put everything together with <code class="language-plaintext highlighter-rouge">grain.DataLoader</code> API, which takes in the raw <code class="language-plaintext highlighter-rouge">dataset</code>, training-required <code class="language-plaintext highlighter-rouge">transformations</code> and <code class="language-plaintext highlighter-rouge">sampler</code>. The returned dataloader is ready to produce batches the downstream training loop needs (<code class="language-plaintext highlighter-rouge">iter(dataloader)</code>).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataloader</span> <span class="o">=</span> <span class="n">grain</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span>
    <span class="n">data_source</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">operations</span><span class="o">=</span><span class="n">transformations</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span>
    <span class="n">worker_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">worker_buffer_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">read_options</span><span class="o">=</span><span class="n">grain</span><span class="p">.</span><span class="nc">ReadOptions</span><span class="p">(</span><span class="n">num_threads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">prefetch_buffer_size</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">data_iter</span> <span class="o">=</span> <span class="nf">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
<span class="n">batch</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">data_iter</span><span class="p">)</span>
</code></pre></div></div> <h3 id="final-words">Final words</h3> <p>Feel free to run and fork <a href="https://colab.research.google.com/drive/1MrIvDAiWcTSma3mDKwmd5F_cxVznfxmQ#scrollTo=VbMbtALpE7om" rel="external nofollow noopener" target="_blank">input_pipeline_data2iter.ipynb</a> if you’d like to run a complete version of input pipeline. It’s worth noting that the returned <code class="language-plaintext highlighter-rouge">batch</code> sits in host CPU memory and so it’s necessary to further shard it across TPU devices before feeding the batch to <code class="language-plaintext highlighter-rouge">pjitted</code> train step. This could be done by <a href="https://github.com/google/maxtext/blob/ead18fbe6f2d8a6cbae6bbd38568146919e20e18/MaxText/multihost_dataloading.py#L93" rel="external nofollow noopener" target="_blank">MultiHostDataLoadIterator</a>. If you’d like to know the details, <a href="/blog/2024/battle-tested-llm-training-multihost-input-pipeline/">this previous post</a> could be of interest. If you’d like to run the input pipeline</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/battle-tested-llm-training-multihost-input-pipeline/">Battle-Tested LLM Training: Multi-host Input Pipeline</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/a-desk-that-listens/">A Desk That Listens</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/a-desk-with-its-own-schedule/">A Desk with Its Own Schedule</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/named-entity-recognition-part2/">Demystifying Named Entity Recognition - Part II</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/named-entity-recognition/">Demystifying Named Entity Recognition - Part I</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Kehang Han. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: August 27, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-battle-tested-llm-training-from-dataset-to-data-iterator",title:"Battle-Tested LLM Training: From Dataset to Data Iterator",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/battle-tested-llm-training-basic-input-pipeline/"}},{id:"post-battle-tested-llm-training-multi-host-input-pipeline",title:"Battle-Tested LLM Training: Multi-host Input Pipeline",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/battle-tested-llm-training-multihost-input-pipeline/"}},{id:"post-a-desk-that-listens",title:"A Desk That Listens",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/a-desk-that-listens/"}},{id:"post-a-desk-with-its-own-schedule",title:"A Desk with Its Own Schedule",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/a-desk-with-its-own-schedule/"}},{id:"post-demystifying-named-entity-recognition-part-ii",title:"Demystifying Named Entity Recognition - Part II",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/named-entity-recognition-part2/"}},{id:"post-demystifying-named-entity-recognition-part-i",title:"Demystifying Named Entity Recognition - Part I",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/named-entity-recognition/"}},{id:"post-molecular-convnet-in-property-prediction",title:"Molecular ConvNet in Property Prediction",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/machine-learning-in-molecular-property-prediction/"}},{id:"post-6-036-project-2-mnist-classifiers",title:"6.036 Project 2: MNIST Classifiers",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/mnist-classifiers-exploration/"}},{id:"post-install-cuda-and-cudnn-on-red-hat",title:"Install CUDA and cuDNN on Red Hat",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/install-CUDA-cuDNN-on-Red-Hat/"}},{id:"post-smart-review-summarization-project",title:"Smart Review Summarization Project",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/smart-review-summarization-project/"}},{id:"post-reactor-optimization-with-fmincon",title:"Reactor Optimization with fmincon!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/fmincon-tutorial/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6B%65%68%61%6E%67%68%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=ON8jkO0AAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>