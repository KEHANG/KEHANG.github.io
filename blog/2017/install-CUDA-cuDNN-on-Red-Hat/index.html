<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Install CUDA and cuDNN on Red Hat | Kehang Han </title> <meta name="author" content="Kehang Han"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kehang.github.io/blog/2017/install-CUDA-cuDNN-on-Red-Hat/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Kehang</span> Han </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Install CUDA and cuDNN on Red Hat</h1> <p class="post-meta"> Created in March 31, 2017 </p> <p class="post-tags"> <a href="/blog/2017"> <i class="fa-solid fa-calendar fa-sm"></i> 2017 </a>   ·   <a href="/blog/category/tools"> <i class="fa-solid fa-tag fa-sm"></i> tools</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I’ve been building neural networks for my chemical space deep learning research since last year. Speedup of training is always one of the central topics. Recently my research group purchased a <code class="language-plaintext highlighter-rouge">Quadro K2200</code> for our Red Hat workstation. I thought it’s a good opportunity of accelerating the computation by switching to GPU. The benefit detail for my research projects will probably be covered in later posts.</p> <p>Today I’m going to focus on how to smoothly install the <strong>two important tools</strong> for any neural network applications to run on GPUs: <strong>CUDA</strong> and <strong>cuDNN</strong>. I noticed there’s serveral installation guides online, e.g., <a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#axzz4cwRN8XWN" rel="external nofollow noopener" target="_blank">NVIDIA official guide</a>, <a href="http://expressionflow.com/2016/10/09/installing-tensorflow-on-an-aws-ec2-p2-gpu-instance/" rel="external nofollow noopener" target="_blank">AWS EC2 guide 1</a> and <a href="http://www.pyimagesearch.com/2016/07/04/how-to-install-cuda-toolkit-and-cudnn-for-deep-learning/" rel="external nofollow noopener" target="_blank">AWS EC2 guide 2</a>. They either are outdated, missing some key steps or contain unneccessary settings. More importantly for beginners, we want to have some tests to see each major step is gone though correctly and neccessarily.</p> <p>For this purpose I decided to create this post, whose goal is to install CUDA and cuDNN on Red Hat Enterprise Linux 7 in a more <strong>transparent</strong> and <strong>reasonable</strong> way.</p> <p>Just to emphasize, my situation was:</p> <ul> <li> <p>I could easily install <code class="language-plaintext highlighter-rouge">theano</code>/<code class="language-plaintext highlighter-rouge">tensorflow</code>/<code class="language-plaintext highlighter-rouge">keras</code> through <code class="language-plaintext highlighter-rouge">anaconda</code> binary platform,</p> </li> <li> <p>my application can already successfully run on CPUs,</p> </li> <li> <p>I only need to make <code class="language-plaintext highlighter-rouge">theano</code>/<code class="language-plaintext highlighter-rouge">tensorflow</code>/<code class="language-plaintext highlighter-rouge">keras</code> detect there’s GPU available</p> </li> </ul> <h2 id="test-examples-prep">Test Examples Prep</h2> <p>I prepared two python test scripts: <code class="language-plaintext highlighter-rouge">example_1</code> is from <a href="http://deeplearning.net/software/theano/tutorial/using_gpu.html" rel="external nofollow noopener" target="_blank">theano official documentation</a>, which is easy and fast to test whether we have connected to GPU. <code class="language-plaintext highlighter-rouge">example_2</code> is from <a href="https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py" rel="external nofollow noopener" target="_blank">keras example cifar10_cnn</a>, which will be used to final check the speedup brought by GPU. Below is the detail of <code class="language-plaintext highlighter-rouge">example_1</code> script.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">theano</span> <span class="kn">import</span> <span class="n">function</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">shared</span><span class="p">,</span> <span class="n">tensor</span>
<span class="kn">import</span> <span class="n">numpy</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">vlen</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="mi">30</span> <span class="o">*</span> <span class="mi">768</span>  <span class="c1"># 10 x #cores x # threads per core
</span><span class="n">iters</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nc">RandomState</span><span class="p">(</span><span class="mi">22</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="nf">shared</span><span class="p">(</span><span class="n">numpy</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">rng</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">vlen</span><span class="p">),</span> <span class="n">config</span><span class="p">.</span><span class="n">floatX</span><span class="p">))</span>
<span class="n">f</span> <span class="o">=</span> <span class="nf">function</span><span class="p">([],</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">maker</span><span class="p">.</span><span class="n">fgraph</span><span class="p">.</span><span class="nf">toposort</span><span class="p">())</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="nf">f</span><span class="p">()</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Looping %d times took %f seconds</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">iters</span><span class="p">,</span> <span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Result is %s</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">r</span><span class="p">,))</span>
<span class="k">if</span> <span class="n">numpy</span><span class="p">.</span><span class="nf">any</span><span class="p">([</span><span class="nf">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">op</span><span class="p">,</span> <span class="n">tensor</span><span class="p">.</span><span class="n">Elemwise</span><span class="p">)</span> <span class="ow">and</span>
              <span class="p">(</span><span class="sh">'</span><span class="s">Gpu</span><span class="sh">'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nf">type</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">op</span><span class="p">).</span><span class="n">__name__</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">f</span><span class="p">.</span><span class="n">maker</span><span class="p">.</span><span class="n">fgraph</span><span class="p">.</span><span class="nf">toposort</span><span class="p">()]):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Used the cpu</span><span class="sh">'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Used the gpu</span><span class="sh">'</span><span class="p">)</span></code></pre></figure> <h2 id="python-anaconda-environment-setup">Python Anaconda Environment Setup</h2> <p>One command can create a conda environment with: <code class="language-plaintext highlighter-rouge">theano</code>. Just run on your terminal:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>conda create <span class="nt">-n</span> theano_test <span class="nt">-c</span> conda-forge theano</code></pre></figure> <h1 id="status-of-test-examples-able-to-run-on-cpus-but-not-gpus">Status of test examples: able to run on CPUs but not GPUs</h1> <p>At this point, we’ll run <code class="language-plaintext highlighter-rouge">example_1</code> script (no need to run <code class="language-plaintext highlighter-rouge">example_2</code>) to make sure <code class="language-plaintext highlighter-rouge">example_1</code> can run on CPUs but not on GPUs.</p> <p>Run <code class="language-plaintext highlighter-rouge">example_1</code> on CPU mode:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">(</span>theano_test<span class="o">)</span> <span class="o">[</span>kehang]<span class="nv">$ </span>python example_1.py</code></pre></figure> <p>Result of <code class="language-plaintext highlighter-rouge">example_1</code> for CPU mode:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>Elemwise<span class="o">{</span>exp,no_inplace<span class="o">}(</span>&lt;TensorType<span class="o">(</span>float64, vector<span class="o">)&gt;)]</span>
Looping 1000 <span class="nb">times </span>took 4.095498 seconds
Result is <span class="o">[</span> 1.23178032  1.61879341  1.52278065 ...,  2.20771815  2.29967753
  1.62323285]
Used the cpu</code></pre></figure> <p>Run <code class="language-plaintext highlighter-rouge">example_1</code> on GPU mode:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">(</span>theano_test<span class="o">)</span> <span class="o">[</span>kehang]<span class="nv">$ THEANO_FLAGS</span><span class="o">=</span><span class="nv">device</span><span class="o">=</span>cuda python example_1.py</code></pre></figure> <p>Result of <code class="language-plaintext highlighter-rouge">example_1</code> for GPU mode:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">ERROR <span class="o">(</span>theano.gpuarray<span class="o">)</span>: Could not initialize pygpu, support disabled
Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:
  File <span class="s2">"/home/kehang/miniconda/envs/keras_test/lib/python2.7/site-packages/theano/gpuarray/__init__.py"</span>, line 164, <span class="k">in</span> &lt;module&gt;
    use<span class="o">(</span>config.device<span class="o">)</span>
  File <span class="s2">"/home/kehang/miniconda/envs/keras_test/lib/python2.7/site-packages/theano/gpuarray/__init__.py"</span>, line 151, <span class="k">in </span>use
    init_dev<span class="o">(</span>device<span class="o">)</span>
  File <span class="s2">"/home/kehang/miniconda/envs/keras_test/lib/python2.7/site-packages/theano/gpuarray/__init__.py"</span>, line 60, <span class="k">in </span>init_dev
    <span class="nv">sched</span><span class="o">=</span>config.gpuarray.sched<span class="o">)</span>
  File <span class="s2">"pygpu/gpuarray.pyx"</span>, line 614, <span class="k">in </span>pygpu.gpuarray.init <span class="o">(</span>pygpu/gpuarray.c:9415<span class="o">)</span>
  File <span class="s2">"pygpu/gpuarray.pyx"</span>, line 566, <span class="k">in </span>pygpu.gpuarray.pygpu_init <span class="o">(</span>pygpu/gpuarray.c:9106<span class="o">)</span>
  File <span class="s2">"pygpu/gpuarray.pyx"</span>, line 1021, <span class="k">in </span>pygpu.gpuarray.GpuContext.__cinit__ <span class="o">(</span>pygpu/gpuarray.c:13468<span class="o">)</span>
GpuArrayException: Error loading library: <span class="nt">-1</span>
<span class="o">[</span>Elemwise<span class="o">{</span>exp,no_inplace<span class="o">}(</span>&lt;TensorType<span class="o">(</span>float64, vector<span class="o">)&gt;)]</span>
Looping 1000 <span class="nb">times </span>took 4.158732 seconds
Result is <span class="o">[</span> 1.23178032  1.61879341  1.52278065 ...,  2.20771815  2.29967753
  1.62323285]
Used the cpu</code></pre></figure> <h2 id="installation-of-cuda-toolkit-and-driver">Installation of CUDA Toolkit and Driver</h2> <p>This part is well documented by <a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#axzz4cwRN8XWN" rel="external nofollow noopener" target="_blank">NVIDIA official guide</a> except that some small steps are either too brief (can be not so actionable) or outdated. This post will cover all the commands step by step in an actionable way, for detailed explanations one can always refer to the official guide.</p> <h1 id="pre-installation-actions">Pre-installation Actions</h1> <p>This step is 100% following <a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pre-installation-actions" rel="external nofollow noopener" target="_blank">NVIDIA official guide: Pre-installation Actions</a>. First run the following commands to verify system requirements are met:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Verify You Have a CUDA-Capable GPU</span>
<span class="nv">$ </span>lspci | <span class="nb">grep</span> <span class="nt">-i</span> nvidia

<span class="c"># Verify You Have a Supported Version of Linux</span>
<span class="nv">$ </span><span class="nb">uname</span> <span class="nt">-m</span> <span class="o">&amp;&amp;</span> <span class="nb">cat</span> /etc/<span class="k">*</span>release

<span class="c"># Verify the System Has gcc Installed</span>
<span class="nv">$ </span>gcc <span class="nt">--version</span>

<span class="c"># Install Correct Kernel Headers and Development Packages for Red Hat</span>
<span class="nv">$ </span><span class="nb">sudo </span>yum <span class="nb">install </span>kernel-devel-<span class="si">$(</span><span class="nb">uname</span> <span class="nt">-r</span><span class="si">)</span> kernel-headers-<span class="si">$(</span><span class="nb">uname</span> <span class="nt">-r</span><span class="si">)</span></code></pre></figure> <p>Now you go to <a href="http://developer.nvidia.com/cuda-downloads" rel="external nofollow noopener" target="_blank">NVIDIA CUDA Toolkit Downloads</a>. Below is what I chose for my Red Hat EL7 machine. It’s a fresh installation so I didn’t have to deal with conflicting previous installations. But for people having previous CUDA installation, please refer to <a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#handle-uninstallation" rel="external nofollow noopener" target="_blank">Handle Conflicting Installation Methods</a>.</p> <p class="cuda_cudnn_img"><img src="/assets/cuda_cudnn_post/img/cuda_download.png" alt="Alt text" width="80%"></p> <h1 id="package-manager-installation">Package Manager Installation</h1> <p>As I mentioned early, I chose to download the <code class="language-plaintext highlighter-rouge">rpm(local)</code> installer option, so I need to use package manager installation. This step we’ll following 80% of the <a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-installation" rel="external nofollow noopener" target="_blank">NVIDIA official guide: Package Manager Installation</a> and add/modify some steps I regard neccessary but not clear in the official guide.</p> <p>For users choosing <code class="language-plaintext highlighter-rouge">runfile</code>, please refer to <a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile" rel="external nofollow noopener" target="_blank">Runfile Installation</a>.</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Add third-party repository EPEL to yum repolist</span>
<span class="nv">$ </span>wget http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-9.noarch.rpm
<span class="nv">$ </span>rpm <span class="nt">-ivh</span> epel-release-7-9.noarch.rpm

<span class="c"># Verify you have EPEL now</span>
<span class="nv">$ </span>yum repolist

<span class="c"># Search in EPEL for dkms and libvdpau </span>
<span class="c"># which are dependencies of CUDA</span>
<span class="nv">$ </span>yum <span class="nt">--enablerepo</span><span class="o">=</span>epel info dkms
<span class="nv">$ </span>yum <span class="nt">--enablerepo</span><span class="o">=</span>epel info libvdpau

<span class="c"># Address custom xorg.conf, if applicable</span>
<span class="c"># I don't have xorg.conf before so it's fine</span>
<span class="c"># if you have, please follow official guide</span>

<span class="c"># Install cuda finally</span>
<span class="c"># takes 10 mins or so</span>
<span class="nv">$ </span><span class="nb">sudo </span>rpm <span class="nt">-i</span> cuda-repo-rhel7-8-0-local-ga2-8.0.61-1.x86_64.rpm
<span class="nv">$ </span><span class="nb">sudo </span>yum clean all
<span class="nv">$ </span><span class="nb">sudo </span>yum <span class="nb">install </span>cuda</code></pre></figure> <h1 id="post-installation-actions">Post-installation Actions</h1> <p>You only need to add <code class="language-plaintext highlighter-rouge">/usr/local/cuda-8.0/bin</code> (version number can vary, please check) to your <code class="language-plaintext highlighter-rouge">PATH</code> environment variable (either by doing as in below or put into your <code class="language-plaintext highlighter-rouge">.bashrc</code> file).</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span>/usr/local/cuda-8.0/bin:<span class="nv">$PATH</span></code></pre></figure> <p>To verify GPUs can be accessed, three small steps are needed</p> <ul> <li> <p>restart the machine</p> </li> <li> <p>verify the driver version by running <code class="language-plaintext highlighter-rouge">cat /proc/driver/nvidia/version</code></p> </li> <li> <p>run <code class="language-plaintext highlighter-rouge">deviceQuery</code> cuda sample binary, in steps as follow</p> </li> </ul> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Copy CUDA samples to your personal directory</span>
<span class="c"># so that you have write permission</span>
<span class="nv">$ </span>cuda-install-samples-8.0.sh &lt;your-target-directory&gt;
<span class="nv">$ </span><span class="nb">cd</span> &lt;your-target-directory&gt;/NVIDIA_CUDA-8.0_Samples

<span class="c"># Compile samples</span>
<span class="nv">$ </span>make

<span class="c"># Running deviceQuery</span>
<span class="nv">$ </span><span class="nb">cd </span>bin/x86_64/linux/release
<span class="nv">$ </span>./deviceQuery</code></pre></figure> <p>Here’s what you’ll get after running <code class="language-plaintext highlighter-rouge">deviceQuery</code>:</p> <p class="cuda_cudnn_img"><img src="/assets/cuda_cudnn_post/img/deviceQuery.png" alt="Alt text" width="80%"></p> <h1 id="status-of-test-examples-able-to-run-on-gpus-but-cudnn-complaints">Status of test examples: able to run on GPUs but cuDNN complaints</h1> <p>Run <code class="language-plaintext highlighter-rouge">example_1</code> on GPU mode:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">(</span>theano_test<span class="o">)</span> <span class="o">[</span>kehang]<span class="nv">$ THEANO_FLAGS</span><span class="o">=</span><span class="nv">device</span><span class="o">=</span>cuda python example_1.py</code></pre></figure> <p>Result of <code class="language-plaintext highlighter-rouge">example_1</code> for GPU mode:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:
/usr/bin/ld: cannot find <span class="nt">-lcudnn</span>
collect2: error: ld returned 1 <span class="nb">exit </span>status

Mapped name None to device cuda: Quadro K2200 <span class="o">(</span>0000:03:00.0<span class="o">)</span>
<span class="o">[</span>GpuElemwise<span class="o">{</span>exp,no_inplace<span class="o">}(</span>&lt;GpuArrayType&lt;None&gt;<span class="o">(</span>float64, <span class="o">(</span>False,<span class="o">))&gt;)</span>, HostFromGpu<span class="o">(</span>gpuarray<span class="o">)(</span>GpuElemwise<span class="o">{</span>exp,no_inplace<span class="o">}</span>.0<span class="o">)]</span>
Looping 1000 <span class="nb">times </span>took 0.535818 seconds
Result is <span class="o">[</span> 1.23178032  1.61879341  1.52278065 ...,  2.20771815  2.29967753
  1.62323285]
Used the gpu</code></pre></figure> <p>Hooray! <code class="language-plaintext highlighter-rouge">example_1</code> is able to access to GPU <code class="language-plaintext highlighter-rouge">Quadro K2200</code> and the wallclock has been reduced by a factor of 8 (from 4.15 sec to 0.53 sec.)</p> <p>But also it shows that <code class="language-plaintext highlighter-rouge">cannot find -lcudnn</code>, which will be our next installation part: <strong>cuDNN installation</strong>.</p> <h2 id="installation-of-cudnn">Installation of cuDNN</h2> <p>This part of installation is relatively easy, and we’ll mainly follow <a href="http://www.pyimagesearch.com/2016/07/04/how-to-install-cuda-toolkit-and-cudnn-for-deep-learning/" rel="external nofollow noopener" target="_blank">AWS EC2 guide 2</a>. But still some steps of it need modified or better explained.</p> <h1 id="cudnn-download">cuDNN Download</h1> <p>To obtain the cuDNN library, one needs</p> <ul> <li>create an account to join <a href="https://developer.nvidia.com/accelerated-computing-developer" rel="external nofollow noopener" target="_blank">NVIDIA developer program</a>.</li> <li>download <a href="https://developer.nvidia.com/rdp/cudnn-download" rel="external nofollow noopener" target="_blank">cuDNN</a> </li> </ul> <p class="cuda_cudnn_img"><img src="/assets/cuda_cudnn_post/img/cudnn_download.png" alt="Alt text" width="80%"></p> <p>I chose <strong><code class="language-plaintext highlighter-rouge">cuDNN Library v5.1 for Linux</code></strong> not <code class="language-plaintext highlighter-rouge">v6.0</code> is because latest <code class="language-plaintext highlighter-rouge">theano</code> can only utilize up to <code class="language-plaintext highlighter-rouge">v5.1</code> (<code class="language-plaintext highlighter-rouge">v6.0</code> has a conflict with <code class="language-plaintext highlighter-rouge">theano</code>, but maybe future <code class="language-plaintext highlighter-rouge">theano</code> can cooperate)</p> <h1 id="cudnn-unpack-and-install">cuDNN Unpack and Install</h1> <p>Copy the download onto your machine, and unpack it</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="nb">tar</span> <span class="nt">-zxf</span> cudnn-8.0-linux-x64-v5.1.tgz</code></pre></figure> <p>Copy cuDNN header file and library files to appropriate <code class="language-plaintext highlighter-rouge">include</code> and <code class="language-plaintext highlighter-rouge">lib64</code> directories.</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># change directory to the unpacked folder of cudnn</span>
<span class="nv">$ </span><span class="nb">cd </span>cuda

<span class="c"># copy related files to /usr/local/cuda/lib64 or /usr/local/cuda/include</span>
<span class="c"># -av will keep the symbolic links as is during copying</span>
<span class="nb">sudo cp</span> <span class="nt">-av</span> lib64/<span class="k">*</span> /usr/local/cuda/lib64/
<span class="nb">sudo cp</span> <span class="nt">-av</span> include/<span class="k">*</span> /usr/local/cuda/include/

<span class="c"># Update your environment variables in bash session</span>
<span class="c"># or put them in your .bashrc file</span>
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda/lib64/:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">export </span><span class="nv">LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda/lib64/:<span class="nv">$LIBRARY_PATH</span></code></pre></figure> <h1 id="status-of-test-examples-run-on-gpus-without-any-complaints">Status of test examples: run on GPUs without any complaints</h1> <p>Run <code class="language-plaintext highlighter-rouge">example_1</code> on GPU mode:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">(</span>theano_test<span class="o">)</span> <span class="o">[</span>kehang]<span class="nv">$ THEANO_FLAGS</span><span class="o">=</span><span class="nv">device</span><span class="o">=</span>cuda python example_1.py</code></pre></figure> <p>Result of <code class="language-plaintext highlighter-rouge">example_1</code> for GPU mode:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">Using cuDNN version 5110 on context None
Mapped name None to device cuda: Quadro K2200 <span class="o">(</span>0000:03:00.0<span class="o">)</span>
<span class="o">[</span>GpuElemwise<span class="o">{</span>exp,no_inplace<span class="o">}(</span>&lt;GpuArrayType&lt;None&gt;<span class="o">(</span>float64, <span class="o">(</span>False,<span class="o">))&gt;)</span>, HostFromGpu<span class="o">(</span>gpuarray<span class="o">)(</span>GpuElemwise<span class="o">{</span>exp,no_inplace<span class="o">}</span>.0<span class="o">)]</span>
Looping 1000 <span class="nb">times </span>took 0.536293 seconds
Result is <span class="o">[</span> 1.23178032  1.61879341  1.52278065 ...,  2.20771815  2.29967753
  1.62323285]
Used the gpu</code></pre></figure> <p>Although <code class="language-plaintext highlighter-rouge">example_1</code> doesn’t show time advantage of using <code class="language-plaintext highlighter-rouge">cuDNN</code> but it clearly shows it’s using cuDNN smoothly.</p> <p>For <code class="language-plaintext highlighter-rouge">example_2</code>, which is a convolutional neural network application, it can run on GPUs as well (below right-corner video). After switching to GPUs, the training process is sppeded up by at least 6 times (from 291 sec/epoch to 45 sec/epoch, I guess you can immediately tell from the progress bar)</p> <iframe width="360" height="215" src="https://www.youtube.com/embed/14sQNwBFv9s" frameborder="0" allowfullscreen=""></iframe> <iframe width="360" height="215" src="https://www.youtube.com/embed/rO1qwGVB47w" frameborder="0" allowfullscreen=""></iframe> <p>But anyways, hope you can enjoy the installation guide of CUDA and cuDNN on Red Hat. You can leave your comments on Youtube if you have any questions or suggestions.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/battle-tested-llm-training-input-pipeline/">Battle-Tested LLM Training: The Input Pipeline</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/a-desk-that-listens/">A Desk That Listens</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/a-desk-with-its-own-schedule/">A Desk with Its Own Schedule</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/named-entity-recognition-part2/">Demystifying Named Entity Recognition - Part II</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/named-entity-recognition/">Demystifying Named Entity Recognition - Part I</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Kehang Han. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: August 04, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-battle-tested-llm-training-the-input-pipeline",title:"Battle-Tested LLM Training: The Input Pipeline",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/battle-tested-llm-training-input-pipeline/"}},{id:"post-a-desk-that-listens",title:"A Desk That Listens",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/a-desk-that-listens/"}},{id:"post-a-desk-with-its-own-schedule",title:"A Desk with Its Own Schedule",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/a-desk-with-its-own-schedule/"}},{id:"post-demystifying-named-entity-recognition-part-ii",title:"Demystifying Named Entity Recognition - Part II",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/named-entity-recognition-part2/"}},{id:"post-demystifying-named-entity-recognition-part-i",title:"Demystifying Named Entity Recognition - Part I",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/named-entity-recognition/"}},{id:"post-molecular-convnet-in-property-prediction",title:"Molecular ConvNet in Property Prediction",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/machine-learning-in-molecular-property-prediction/"}},{id:"post-6-036-project-2-mnist-classifiers",title:"6.036 Project 2: MNIST Classifiers",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/mnist-classifiers-exploration/"}},{id:"post-install-cuda-and-cudnn-on-red-hat",title:"Install CUDA and cuDNN on Red Hat",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/install-CUDA-cuDNN-on-Red-Hat/"}},{id:"post-smart-review-summarization-project",title:"Smart Review Summarization Project",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/smart-review-summarization-project/"}},{id:"post-reactor-optimization-with-fmincon",title:"Reactor Optimization with fmincon!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/fmincon-tutorial/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6B%65%68%61%6E%67%68%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=ON8jkO0AAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>